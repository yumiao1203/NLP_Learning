#### Attention机制

ICLR 

**NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE（2015）** 

**Effective Approaches to Attention-based Neural Machine Translation（2015）** 

\- 基本的Attention原理 

Attention机制通俗的讲就是把注意力集中放在重要的点上。根据场景不同场景，Attention分为空间注意力和时间注意力，前者用于图像处理，后者用于nlp。主要介绍Attention机制在Seq2seq中的应用。 

\- HAN的原理（Hierarchical Attention Networks） 