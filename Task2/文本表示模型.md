### 文本表示模型

文本表示的意思是把字词处理成向量或矩阵，以便计算机能进行处理。文本表示是自然语言处理的开始环节。

文本表示按照颗粒度划分，一般可分为字级别、词语级别和句子级别的文本表示。字级别（char level）的如把“邓紫棋实在太可爱了，我想养一只”这句话拆成一个个的字：｛邓，紫，棋，实，在，太，可，爱，了，我，想，养，一，只｝，然后把每个字用一个向量表示，那么这句话就转化为了由14个向量组成的矩阵。

文本表示分为离散表示和分布式表示。**离散表示**的代表是词袋模型，one-hot（也叫独热编码）、TF-IDF、n-gram都可以看作是词袋模型。**分布式表示**也叫做词嵌入（word embedding ），经典模型是word2vec，还包括后来的Glove、ELMO、GPT和最近很火的BERT。

#### 离散表示--词袋模型

假如现在有1000篇新闻文档，把这些文档**拆成一个个的字**，去重后得到3000个字，然后把这**3000个字作为字典**，进行文本表示的模型，叫做词袋模型。这种模型的特点是字典中的字没有特定的顺序，句子的总体结构也被舍弃了。下面分别介绍词袋模型中的one-hot、TF-IDF和n-gram文本表示方法。

##### （一）one-hot

两句话：

“邓紫棋太可爱了，我爱邓紫棋”

“邓紫棋太可爱了，我爱邓紫棋”

把这两句话拆成一个个的字，整理得到14个不重复的的字，这14个字决定了在文本表示时向量的长度为14。

| 邓   | 紫   | 棋   | 太   | 可   | 爱   | 了   | 我   | 要   | 看   | 的   | 演   | 唱   | 会   |
| ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| 1    | 1    | 1    | 1    | 1    | 1    | 1    | 1    | 0    | 0    | 0    | 0    | 0    | 0    |
| 1    | 1    | 1    | 0    | 0    | 0    | 0    | 1    | 1    | 1    | 1    | 1    | 1    | 1    |

其中表格的二、三行就是上面两句话的one-hot表示。

**one-hot的问题**

尽管句子长度不一样，但是通过one-hot表示的向量有相同的长度，方便进行矩阵运算。但也存在以下问题：

1.数据稀疏和维度灾难。如果词袋中的字词达数百万个。那么每篇文档去重后的字数较少，因此向量中的绝大部分的元素是0。

2.没有考虑句中字的顺序性，假定字之间相互独立。“邓紫棋爱我”“我爱邓紫棋”的向量表示是一样的。

3.没有考虑字的相对重要性，只考虑字的出现与否，不管出现的频率。

#### 分布式表示

##### word2vec

谷歌2013年提出的Word2vec就是词嵌入模型之一，词嵌入是将词向量化的模型的通称，其**核心思想**是将每个词映射成低维-K维空间(通常K=50~300)的一个稠密向量(Dense Vector)。K维空间的每一维都可以看作一个隐含的主题，只不过不像主题模型中的主题那样直观。

词嵌入将每个词映射成K维向量，每篇文档假设有N个词，则这篇文档就可以用N x K的矩阵表示，但是这样的表示太底层化。在实际的应用中，如果将这个矩阵作为原文本的表示特征输入到机器学习模型中，很难达到令人满意的结果。因此需要在在此基础上加工出更高层的特征。

在传统的浅层机器学习模型中，一个好的特征工程往往可以带来算法效果的显著提升，而深度学习模型则可以为我们提供一种**自动化地进行特征工程的方式**，模型的每个隐层都可以认为对应着不同抽象层次的特征。从这个角度来讲，深度学习模型打败浅层模型也就顺理成章了。

word2vec作为神经概率语言模型的输入，其本身其实是神经概率模型的副产品，是为了通过神经网络学习**某个语言模型**而产生的中间结果。具体来说，“某个语言模型”指的是“CBOW”和“Skip-gram”。具体学习过程会用到两个降低复杂度的近似方法——Hierarchical Softmax或Negative Sampling。两个模型乘以两种方法，一共有四种实现。这些内容就是本文理论部分要详细阐明的全部了。

###### CBOW
![avatar](png/CBOW.png)

CBOW是 Continuous Bag-of-Words Model的缩写，是一种根据上下文的词语预测当前词语的出现概率模型。CBOW是已知上下文，估算当前词语的语言模型。其学习目标是最大化对数似然函数：

$$L=\sum_{\omega\in C}logp(\omega|Context(\omega))$$

其中，$\omega$表示语料库$C$中的任意一个词下面，以样本$(Context(\omega),\omega)$为例（这里假设$Context(\omega)$指$\omega$的前后各$c$个词构成），从上图可以看出，对于CBOW

**输入层** 

是包含$Context(\omega)$中$2c$个词的词向量$v((Context(\omega)_1)$,$v((Context(\omega)_2)$,…,$v((Context(\omega)_{2c})\in\mathbb{R}^m$,这里，m表示词向量的长度。（什么！我们不是在训练词向量吗？不不不，我们是在训练CBOW模型，词向量只是个副产品，确切来说，是CBOW模型的一个参数。训练开始的时候，词向量是个随机值，随着训练的进行不断被更新）

**投影层** 

将输入层的$2c$个向量做求和累加，即$x_\omega = \sum_{i=1}^{2c}v(Context(\omega)_{i}) \in \mathbb{R}^m $

![avatar](png/CBOW网络结构示意图.png)



**输出层** 输出最有可能的$\omega$，由于语料库中词汇量是固定的|C|个。所以上述过程可以看作一个多分类问题。给定特征，从|C|个分类中挑一个。

对于神经网络模型多分类，最朴素的做法是softmax回归，softmax需要对语料库中的每个词语都计算一遍概率并进行归一化，几十万词汇量的语料上无疑令人头疼。

###### 基于Hierarchical Softmax的模型概述













参考文献：
http://www.hankcs.com/nlp/word2vec.html
https://www.cnblogs.com/Luv-GEM/p/10593103.html
https://www.cnblogs.com/peghoty/p/3857839.html



















